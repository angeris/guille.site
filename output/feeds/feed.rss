<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Longest Path Search</title><link>https://guille.site/</link><description></description><lastBuildDate>Sat, 15 Dec 2018 00:00:00 -0800</lastBuildDate><item><title>Physics, optimization, and impossibility</title><link>https://guille.site/physics-impossibility-results.html</link><description>&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; this post is based on the results of &lt;a href="https://arxiv.org/abs/1811.12936"&gt;this arXiv paper&lt;/a&gt; which I've been working on with Stephen Boyd and Jelena Vuckovic.&lt;/p&gt;
&lt;p&gt;The main result of the above paper is kind of weird: essentially, it turns out that you can say what devices are physically &lt;em&gt;impossible&lt;/em&gt; by phrasing certain …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Guillermo Angeris</dc:creator><pubDate>Sat, 15 Dec 2018 00:00:00 -0800</pubDate><guid isPermaLink="false">tag:guille.site,2018-12-15:/physics-impossibility-results.html</guid><category>statistics</category><category>math</category><category>physics</category></item><item><title>Markov processes and the second law</title><link>https://guille.site/second-law-markov.html</link><description>&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; This is another one of those "quick" posts about a topic I've found to be fascinating, but which is almost never discussed.&lt;/p&gt;
&lt;p&gt;Physics has this nice little law called the &lt;a href="https://en.wikipedia.org/wiki/Second_law_of_thermodynamics"&gt;second law of thermodynamics&lt;/a&gt;, which governs every physical thermodynamical system in question. The second law is usually phrased as …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Guillermo Angeris</dc:creator><pubDate>Thu, 13 Sep 2018 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:guille.site,2018-09-13:/second-law-markov.html</guid><category>statistics</category><category>math</category><category>information-theory</category><category>physics</category><category>quick-post</category></item><item><title>Machine learning, information, and tail bounds</title><link>https://guille.site/ml-information-bounds.html</link><description>&lt;p&gt;Usually, in explaining the connection between information theory and machine learning, I would begin by writing down the definition of entropy and deriving some useful results about it, and then come back to tell you that you can look at ML as an information problem, where nature picks some parameters …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Guillermo Angeris</dc:creator><pubDate>Tue, 04 Sep 2018 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:guille.site,2018-09-04:/ml-information-bounds.html</guid><category>statistics</category><category>math</category><category>information-theory</category></item><item><title>PCA as a convex optimization problem</title><link>https://guille.site/pca-convex.html</link><description>&lt;p&gt;It's been a while since I last posted (my posting has been less once every two weeks and more like one every two months), but here's a post I've been sitting on for a while that I never got around to finishing. As per &lt;a href="https://rachelbythebay.com/w/2018/03/13/write/"&gt;rachelbythebay's advice&lt;/a&gt;, I decided to just …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Guillermo Angeris</dc:creator><pubDate>Wed, 16 May 2018 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:guille.site,2018-05-16:/pca-convex.html</guid><category>math</category><category>pca</category><category>non-convex</category><category>convex</category></item><item><title>Fast shortest paths for time-varying graphs (Part 4/?)</title><link>https://guille.site/path-optimization-thoughts4.html</link><description>&lt;p&gt;This is the fourth post in a series of posts describing an approach to doing path-planning in real-time on a small, embedded compute board. This is yet another relatively standalone post which mostly describes how to generate a (starting) path used in the &lt;a href="/path-optimization-thoughts2.html"&gt;second&lt;/a&gt; and &lt;a href="/path-optimization-thoughts.html"&gt;first&lt;/a&gt; posts to generate a …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Guillermo Angeris</dc:creator><pubDate>Sat, 17 Mar 2018 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:guille.site,2018-03-17:/path-optimization-thoughts4.html</guid><category>control-theory</category><category>math</category><category>non-convex</category><category>path-planning</category><category>graph-theory</category><category>auvsi</category></item><item><title>Proximal gradient for SVM</title><link>https://guille.site/svm-prox.html</link><description>&lt;p&gt;For a class that's currently being written (&lt;em&gt;ahem&lt;/em&gt;, EE104), Prof. Boyd posed an interesting problem of writing a (relatively general, but ideally simple) proximal-gradient optimizer. The idea is that would act as a black-box way for students to plug in machine learning models of a specific form and have the …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Guillermo Angeris</dc:creator><pubDate>Fri, 22 Dec 2017 00:00:00 -0800</pubDate><guid isPermaLink="false">tag:guille.site,2017-12-22:/svm-prox.html</guid><category>machine-learning</category><category>math</category><category>optimization-methods</category></item><item><title>Optimizers, momentum, and cooling schedules (Part 2/?)</title><link>https://guille.site/path-optimization-thoughts2.html</link><description>&lt;p&gt;This is the second post in a series of posts describing an initial approach to doing path-planning in real-time on a small, embedded compute board. For the first in the series which describes the energy function used below, see the &lt;a href="/path-optimization-thoughts.html"&gt;first post&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Quick recap&lt;/h2&gt;
&lt;p&gt;Anyways, we left off on the …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Guillermo Angeris</dc:creator><pubDate>Sun, 22 Oct 2017 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:guille.site,2017-10-22:/path-optimization-thoughts2.html</guid><category>control-theory</category><category>math</category><category>non-convex</category><category>path-planning</category><category>auvsi</category></item><item><title>Some thoughts on global path optimization (Part 1/?)</title><link>https://guille.site/path-optimization-thoughts.html</link><description>&lt;p&gt;I usually see path planning in some shape or form usually solved as a Bellman update, Dynamic Programming-style problem, where the given control is asymptotically stable and optimal; in general, this seems to work quite well, but when we have so much computational power available now-a-days, I do wonder if …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Guillermo Angeris</dc:creator><pubDate>Tue, 17 Oct 2017 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:guille.site,2017-10-17:/path-optimization-thoughts.html</guid><category>control-theory</category><category>math</category><category>non-convex</category><category>path-planning</category><category>auvsi</category></item><item><title>Least-squares and image processing</title><link>https://guille.site/ls-images.html</link><description>&lt;p&gt;Least squares is one of those things that seems relatively simple once you first look at it (perhaps also because most linear algebra texts relegate it to nothing more than a page or two on their textbooks), but has surprisingly powerful implications that are quite nice, and, most importantly, that …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Guillermo Angeris</dc:creator><pubDate>Tue, 19 Sep 2017 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:guille.site,2017-09-19:/ls-images.html</guid><category>math</category><category>least-squares</category><category>image-processing</category></item><item><title>PID as least squares</title><link>https://guille.site/pid-ls.html</link><description>&lt;p&gt;I want to say this is a folk theorem (borrowing terminology from game theory) in that everyone who does optimal control theory knows about this stuff, probably,&lt;sup id="fnref-people"&gt;&lt;a class="footnote-ref" href="#fn-people"&gt;1&lt;/a&gt;&lt;/sup&gt; but I haven't really seen it stated explicitly anywhere. If anyone does indeed work on optimal control, I'd love to know your …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Guillermo Angeris</dc:creator><pubDate>Wed, 13 Sep 2017 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:guille.site,2017-09-13:/pid-ls.html</guid><category>control-theory</category><category>math</category><category>least-squares</category></item><item><title>This blog is a lie</title><link>https://guille.site/blog-lie.html</link><description>&lt;p&gt;(Maybe.)&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Guillermo Angeris</dc:creator><pubDate>Tue, 12 Sep 2017 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:guille.site,2017-09-12:/blog-lie.html</guid></item></channel></rss>