<!DOCTYPE html>
<html lang="en">
<head>
    <title>Physics, optimization, and impossibility</title>
    
    <link rel="stylesheet" href="/theme/css/main.css">

    <!-- <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ['\\(','\\)'] ]
            },
        });
    </script>
    
    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script> -->
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">    	

</head>
<body>
    <div class="container">
<div class="row">
    <div class="col-md-8">
        <h2><a href="/">&larr; Home</a></h2>
        <h1>Physics, optimization, and impossibility</h1>
        <p>Category: <a href="/category/physics.html">physics</a></p>
        <p><label>Posted <strong>December 17, 2018</strong></label></p>
        <p><em>Note:</em> this post is based on the results of <a href="https://arxiv.org/abs/1811.12936">this arXiv paper</a> which I've been working on with Stephen Boyd and Jelena Vuckovic.</p>
<p>The main result of the above paper is kind of weird: essentially, it turns out that you can say what devices are physically <em>impossible</em> by phrasing certain problems as optimization problems and then using some basic tools of optimization to derive lower bounds.</p>
<p>To illustrate: imagine you want to generate an engine which is as efficient as possible, then we know the best you could possibly hope to do is given by the <a href="https://en.wikipedia.org/wiki/Second_law_of_thermodynamics">second law of thermodynamics</a>. Now, what if (and bear with me here) we want something a little weirder? Say, what if we want a heat sink that has a particular dissipation pattern? Or what if you want a photonic crystal that traps light of a given wavelength in some region? Or a horn which has specific resonances?</p>
<p>We can write down the optimization problems corresponding to each of these circumstances: in general, these problems are very hard to solve in ways that aren't just "try all possible designs and pick the best one." (And there are a <em>lot</em> of possible designs.) On the other hand, by using some simple heuristics—gradient descent, for example—we appear to give much better devices than what almost anyone can do by hand. This approach, while it appears to work well in practice, brings up a few questions with no obvious answers.</p>
<ol>
<li>Maybe there is some design that is really complicated that these heuristics almost always miss, but that is much better than the current ones.</li>
<li>It is possible that the objective we are requesting is physically impossible to achieve and we will never find a good design.</li>
<li>Many heuristics depend heavily on the initial design we provide. Physical intuition sometimes appears to provide good initializations, but often the final design is unintuitive, so perhaps there are better approaches.</li>
</ol>
<p>The paper provides (some) answers to these questions. In particular, it answers point (2) as its main goal, which gives a partial answer to (1) (namely that the heuristics we use appear to give designs that are often close to the best possible design, at least for the problems we tested), and an answer to (3), since the impossibility result suggests an initial design as a byproduct of computing the certificate of impossibility.</p>
<p>I'll explain the interesting parts of this paper in more detail below, since the paper (for the sake of brevity) simply references the reader to derivations of the results (and leaves some as exercises).</p>
<h2>Lagrange duality</h2>
<p>In optimization theory, there is a beautiful idea called <em>Lagrange duality</em>, which gives lower bounds to any optimization problem you can write down (at least theoretically speaking).</p>
<p>Let's say we have the following optimization problem,
</p>
<div class="math">$$
\begin{array}{ll}
\text{minimize} &amp; f(x)\\
\text{subject to} &amp; h(x) \le 0,
\end{array}
$$</div>
<p>
(this encompasses essentially every optimization problem ever) with objective function <span class="math">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> and constraint function <span class="math">\(h: \mathbb{R}^n \to \mathbb{R}^m\)</span>, where the inequality is taken elementwise. Call the optimal value of the objective of the optimization problem <span class="math">\(p^\star\)</span>, which we will see again soon.</p>
<p>Continuing, we can then formulate the <em>Lagrangian</em> of the problem,
</p>
<div class="math">$$
\mathcal{L}(x, \lambda) = f(x) + \lambda^Th(x),
$$</div>
<p>
with <span class="math">\(\lambda \ge 0\)</span>. Finally, we formulate the dual <em>function</em>
</p>
<div class="math">$$
g(\lambda) = \inf_x \left(f(x) + \lambda^Th(x)\right).
$$</div>
<p>
Now, and here's the magic, this dual function <span class="math">\(g(\lambda)\)</span> at any <span class="math">\(\lambda \ge 0\)</span> is always a lower bound for the optimal objective <span class="math">\(p^\star\)</span>. Why? Well,
</p>
<div class="math">$$
g(\lambda) = \inf_x \mathcal{L}(x, \lambda),
$$</div>
<p>
and, by definition of <span class="math">\(\inf\)</span>,
</p>
<div class="math">$$
\inf_x \mathcal{L}(x, \lambda) \le \mathcal{L}(x, \lambda),
$$</div>
<p>
for every <span class="math">\(x\)</span>. Now, every feasible point <span class="math">\(x^\mathrm{feas}\)</span> of the optimization problem satisfies <span class="math">\(h(x) \le 0\)</span> (this is the definition of 'feasible'), so, since <span class="math">\(\lambda \ge 0\)</span>,
</p>
<div class="math">$$
\mathcal{L}(x^\mathrm{feas}, \lambda) = f(x^\mathrm{feas}) + \underbrace{\lambda^Th(x^\mathrm{feas})}_{\le 0} \le f(x^\mathrm{feas}).
$$</div>
<p>
In other words, for any feasible point, <span class="math">\(\mathcal{L}(x^\mathrm{feas}, \lambda)\)</span> is always smaller than the objective value at that point. But, since <span class="math">\(g(\lambda)\)</span> is smaller than <span class="math">\(\mathcal{L}(x, \lambda)\)</span> for <em>any</em> <span class="math">\(x\)</span>, not just the feasible ones, we have
</p>
<div class="math">$$
g(\lambda) \le f(x^\mathrm{feas}),
$$</div>
<p>
for any feasible point. This means that it is also at most as large as the optimal value (since every optimal point of this optimization problem is feasible). That is,
</p>
<div class="math">$$
g(\lambda) \le p^\star.
$$</div>
<p>
Therefore, for any <span class="math">\(\lambda\ge 0\)</span>, we know that <span class="math">\(g(\lambda)\)</span> is always a lower bound to the optimal objective value!</p>
<p>Of course, sometimes computing <span class="math">\(g(\lambda)\)</span> is at least as difficult as solving the original problem (due to the <span class="math">\(\inf\)</span> we have in the definition of <span class="math">\(g\)</span>). It just so happens that many physical equations and objectives we care about are of a form elegant enough to give an explicit formula for <span class="math">\(g\)</span>, which is the main point of this paper.</p>
<h3>Best lower bound</h3>
<p>Of course, we often want the best (largest) lower bound, not just <em>a</em> lower bound (which can often be quite bad). In other words, we want to maximize our lower bound. We can phrase this as the new optimization problem,
</p>
<div class="math">$$
\begin{array}{ll}
\text{maximize} &amp; g(\lambda)\\
\text{subject to} &amp; \lambda \ge 0.
\end{array}
$$</div>
<p>
What is interesting is that this optimization problem is always convex—<em>e.g.</em>, it is almost always easy to compute the optimal value, <em>if</em> we can explicitly write down what <span class="math">\(g\)</span> is. (I won't prove this here, but the proof is very straightforward. Take a peek at section 5.1.2 in Boyd's <a href="https://web.stanford.edu/~boyd/cvxbook/"><em>Convex Optimization</em></a>.)</p>
<h2>Explicit dual function for <span class="math">\(g\)</span></h2>
<h3>Problem formulation</h3>
<p>Many of the problems we're interested in (including design in photonics via Maxwell's equations,<sup id="fnref-maxwell"><a class="footnote-ref" href="#fn-maxwell">1</a></sup> acoustics via Helmholtz's equations, quantum mechanics via Schrodinger's equation, and heat engineering via the heat equation) have physics equations of the form (once discretized)
</p>
<div class="math">$$
(A + \mathrm{diag}(\theta))z = b,
$$</div>
<p>
where <span class="math">\(\theta \in \mathbb{R}^n\)</span> are the design parameters (<em>e.g.</em> permittivity in the case of photonics, or speed of sound in the material in the case of acoustics) and <span class="math">\(z \in \mathbb{R}^n\)</span> is the field (<em>e.g.</em> the electric field in photonics, or the amplitude of the wave in acoustics). <span class="math">\(A \in \mathbb{R}^{n\times n}\)</span> is a matrix encoding the physics (the curl of the curl in Maxwell's equations, or a discretized Laplacian in Helmholtz's) and <span class="math">\(b \in \mathbb{R}^n\)</span> is an excitation of the field.</p>
<p>More specifically, take a peek at Helmholtz's equation:
</p>
<div class="math">$$
\nabla^2 a(x) + \left(\frac{\omega^2}{c(x)^2}\right)a(x) = u(x),
$$</div>
<p>
where <span class="math">\(c: \mathbb{R}^3 \to \mathbb{R}\_{&gt; 0}\)</span> is a function specifying the speed of sound at every point in the material, while <span class="math">\(a: \mathbb{R}^3 \to \mathbb{R}\)</span> is a function specifying the amplitude at each point, <span class="math">\(u: \mathbb{R}^3 \to \mathbb{R}\)</span> is a function specifying an excitation, and <span class="math">\(\omega \in \mathbb{R}\_{\ge 0}\)</span> is the frequency of the wave. We can make some simple correspondences:
</p>
<div class="math">$$
\Bigg(\underbrace{\nabla^2}\_{A} + \underbrace{\bigg(\frac{\omega^2}{c(x)^2}\bigg)}\_{\mathrm{diag}(\theta)}\Bigg)\underbrace{a(x)}\_{z} = \underbrace{u(x)}\_{b}.
$$</div>
<p>Now, we usually want the field (<span class="math">\(z\)</span>) to look similar to a desired field (which we will call <span class="math">\(\hat z\)</span>), while satisfying the physics equation described above. We can phrase this in several ways, but a <a href="/ls-images.html">particularly natural one</a> is by attempting to minimize the objective <span class="math">\(\left\\|z - \hat z\right\\|\_2^2\)</span>.</p>
<p>Finally, we are only able to choose materials within a specific range: that is, <span class="math">\(\theta^\mathrm{min} \le \theta \le \theta^\mathrm{max}\)</span>.</p>
<p>Putting all of this together, we can write the optimization problem as
</p>
<div class="math">$$
\begin{array}{ll}
\text{minimize} &amp; \frac12\left\\|z - \hat z\right\\|\_2^2\\
\text{subject to} &amp; (A + \mathrm{diag}(\theta))z = b\\
&amp; \theta^\mathrm{min} \le \theta \le \theta^\mathrm{max}.
\end{array}
$$</div>
<p>
which is exactly problem (1) in the paper, in the special case where <span class="math">\(W = I\)</span>, the identity matrix.</p>
<h3>Deriving the dual</h3>
<p>Here is essentially the only 'magic' part of the paper. First, we can write the Lagrangian of the problem as,
</p>
<div class="math">$$
\mathcal{L}(z, \theta, \nu) = \frac12\left\\|z - \hat z\right\\|\_2^2 + \nu^T((A + \mathrm{diag}(\theta))z - b).
$$</div>
<p>
Now, there is something weird here: notice that I sneakily dropped the term containing the lower and upper limits for <span class="math">\(\theta\)</span>—this idea is, in fact, what saves the entire approach. What we will first do is the usual thing: we'll minimize the Lagrangian over all possible <span class="math">\(z\)</span>, which we can easily do since the Lagrangian is a convex quadratic over <span class="math">\(z\)</span>. In particular, taking the gradient over <span class="math">\(z\)</span> and setting it to zero (which is necessary and sufficient by convexity and differentiability) gives us that the optimal <span class="math">\(z\)</span> is
</p>
<div class="math">$$
z = \hat z - (A + \mathrm{diag(\theta)})^T\nu,
$$</div>
<p>
which means that
</p>
<div class="math">$$
\inf_z \mathcal{L}(z, \theta, \nu) = - \frac12\left\\|\hat z - (A + \mathrm{diag(\theta)})^T\nu\right\\|\_2^2 - \nu^Tb + \frac12\\|\hat z\\|\_2^2.
$$</div>
<p>
The next step is then finding the infimum of <span class="math">\(\mathcal{L}\)</span> over <span class="math">\(\theta\)</span>. That is, finding
</p>
<div class="math">$$
\inf_\theta \left(\inf_z \mathcal{L}(z, \theta, \nu)\right).
$$</div>
<p>
Now, of course, minimization over all <span class="math">\(\theta\)</span> is a lower bound (but not a very good one), since, unless <span class="math">\(\nu = 0\)</span>, we can send the whole thing to negative infinity. (Why?)</p>
<p>What we can do instead is minimize over <span class="math">\(\theta\)</span>, constrained to its feasible range, <span class="math">\(\theta^\mathrm{min} \le \theta \le \theta^\mathrm{max}\)</span>. I'll leave it as an exercise for the reader as to why this is still a lower bound, but you should ponder this very carefully, because it is the main point of the paper. As an initial hint, take a second look at the proof above for why the Lagrangian is a lower bound in the first place. Of course, a second hint can be found in the paper which gives a somewhat-natural construction (sometimes called the "partial Lagrangian"), but I highly recommend sitting down with a bit of wine (or something stronger) and thinking about it!<sup id="fnref-wine"><a class="footnote-ref" href="#fn-wine">2</a></sup></p>
<p>If you've convinced yourself of this (or haven't yet, but want to continue), we now have the following minimization problem:
</p>
<div class="math">$$
\begin{aligned}
g(\nu) &amp;= \inf_{\theta^\mathrm{min} \le \theta \le \theta^\mathrm{max}} \left(\inf_z \mathcal{L}(z, \theta, \nu)\right)\\
&amp;= \inf_{\theta^\mathrm{min} \le \theta \le \theta^\mathrm{max}} \left(- \frac12\left\\|\hat z - (A + \mathrm{diag(\theta)})^T\nu\right\\|\_2^2 - \nu^Tb + \frac12\\|\hat z\\|\_2^2\right)
\end{aligned}
$$</div>
<p>
The trick is to notice two things. One, that the objective is concave in <span class="math">\(\theta\)</span> and, two, that the objective is <em>separable</em> over each component of <span class="math">\(\theta\)</span>. </p>
<p>First off, let's say a function <span class="math">\(v: \mathbb{R} \to \mathbb{R}\)</span> is concave over the interval <span class="math">\([L, U]\)</span>, then it achieves its minimum value at the boundaries of the interval. Why? Well, the definition of concavity says, for every <span class="math">\(0 \le \gamma \le 1\)</span>,
</p>
<div class="math">$$
v(\gamma L + (1- \gamma)U) \ge \gamma v(L) + (1-\gamma)v(U) \ge \min\\{v(L), v(U)\\}.
$$</div>
<p>
but any point in the interval <span class="math">\([L, U]\)</span> is a convex combination of <span class="math">\(L\)</span> or <span class="math">\(U\)</span>! So every point inside of the interval is at least as large as the smallest endpoint of the interval, which completes the proof.</p>
<p>This solves our problem: since the objective is separable, then we only need to consider each component of <span class="math">\(\theta\)</span>, and, because it's concave, then we know that an optimal <span class="math">\(\theta_i\)</span> is one of either <span class="math">\(\theta^\mathrm{min}_i\)</span> or <span class="math">\(\theta^\mathrm{max}_i\)</span>. Replacing the complicated <span class="math">\(\inf\)</span> with this (much simpler) <span class="math">\(\min\)</span> gives the analytic solution for <span class="math">\(g\)</span>:
</p>
<div class="math">$$
\begin{multline}
g(\nu) = \sum_i \min\bigg\\{-\frac12 (\hat z\_i - a\_i^T\nu + \theta\_i^\mathrm{min} \nu_i)^2, - \frac12 (\hat z\_i - a\_i^T\nu + \theta\_i^\mathrm{max} \nu_i)^2\bigg\\} \\- \nu^Tb + \frac12\\|\hat z\\|\_2^2,
\end{multline}
$$</div>
<p>
or, writing it in the same way as the paper, by pulling out the <span class="math">\(-1/2\)</span> (and using <span class="math">\(\theta^\mathrm{min} = 0\)</span>),
</p>
<div class="math">$$
g(\nu) = -\frac12 \sum_i \max\bigg\\{ (\hat z\_i - a\_i^T\nu + \nu_i)^2, (\hat z\_i - a\_i^T\nu + \theta\_i^\mathrm{max} \nu_i)^2\bigg\\} - \nu^Tb + \frac12\\|\hat z\\|\_2^2.
$$</div>
<p>
Now that we have an analytic form for <span class="math">\(g\)</span> (our set of lower bounds), we can maximize the function to get the best lower bound. As discussed before, this is a convex optimization problem which can be formulated by <a href="https://www.cvxpy.org">CVXPY</a> and solved using one of the many available solvers for <a href="https://osqp.org">convex quadratically-constrained quadratic programs</a> (QCQPs) or <a href="https://www.embotech.com/ECOS">second-order conic programs</a> (SOCPs).</p>
<h2>Results</h2>
<p>I'll give a quick summary of the results of the paper, but this is the section I would recommend checking out in the paper itself. (There are pretty pictures!)</p>
<p>For a relatively complex design, we found that a simple, commonly used heuristic finds a design with an objective value lying around 9% above the lower bound, and, therefore has objective value at most 9% above the <em>best possible</em> design. (In general, though, we suspect that the true optimum lies closer to the designs that the heuristics give than the lower bound we come up with.) In other words, it is physically impossible to more-than-marginally improve upon this design.</p>
<p>Additionally (I might discuss how this is done in a later post), we receive an initial design that is qualitatively quite similar to the final, locally-optimized design. See the image below.</p>
<p><img src="/images/physics-impossibility-results/primal-dual-comparison.png" class="insert" style="width: 100%">
<em>Comparison between the design suggested by the lower bound and the locally-optimized design.</em></p>
<p>I highly recommend checking out the <a href="https://arxiv.org/abs/1811.12936">pre-print that is up on arXiv</a> for more info. Also, if you spot any mistakes (in either the post or the paper), please do @ me!</p>
<div class="footnote">
<hr>
<ol>
<li id="fn-maxwell">
<p>This is... almost accurate, but not quite. It turns out a small modification to the problem is needed for Maxwell's equations in two and three dimensions. For specifics, see the appendix in the paper.&#160;<a class="footnote-backref" href="#fnref-maxwell" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn-wine">
<p>Honestly, I <em>only</em> recommend reading this blog with wine (or whatever you have at hand). Not sure it's bearable, otherwise.&#160;<a class="footnote-backref" href="#fnref-wine" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'blue ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        <hr>
        <div class="socials">
            <li>Got any questions?</li>
            <li><a href="https://twitter.com/intent/tweet?screen_name=guilleangeris" class="twitter-mention-button" data-size="large" data-show-count="false">Tweet to @guilleangeris</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></li>
        </div>
    </div>
</div>
    </div>
</body>
<footer>
    Made with <a href="https://blog.getpelican.com">🐍</a> and hosted on <a href="https://neocities.org">neocities.org</a>.
</footer>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-73543332-2', 'auto');
    ga('send', 'pageview');
</script>
</html>