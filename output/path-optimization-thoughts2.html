<!DOCTYPE html>
<html lang="en">
<head>
    <title>Optimizers, momentum, and cooling schedules (Part 2/?)</title>
    
    <link rel="stylesheet" href="/theme/css/main.css">

    <!-- <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ['\\(','\\)'] ]
            },
        });
    </script>
    
    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script> -->
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">    	

</head>
<body>
    <div class="container">
<div class="row">
    <div class="col-md-8">
        <h2><a href="/">&larr; Home</a></h2>
        <h1>Optimizers, momentum, and cooling schedules (Part 2/?)</h1>
        <p>Category: <a href="/category/auvsi-competition.html">auvsi-competition</a></p>
        <p><label>Posted <strong>October 22, 2017</strong></label></p>
        <p>This is the second post in a series of posts describing an initial approach to doing path-planning in real-time on a small, embedded compute board. For the first in the series which describes the energy function used below, see the <a href="/path-optimization-thoughts.html">first post</a>.</p>
<h2>Quick recap</h2>
<p>Anyways, we left off on the idea that we now have a function which we wish to optimize, along with a sequence of constants <span class="math">\(C\)</span> which tends to a given solution‚Äîin particular, and perhaps most importantly, we only care about the solution in the limit <span class="math">\(C\to\infty\)</span>.</p>
<p>As before, though, we can't just optimize the function
</p>
<div class="math">$$
\mathcal{L}(x; c, R, C) = \sum_{i}\left[\sum_j\phi\left(C\left(\frac{\lVert x_i - c_j \lVert_2^2}{R_j^2} - 1\right)\right) + \eta \lVert x_i - x_{i+1}\lVert_2^2\right]
$$</div>
<p>over some large <span class="math">\(C\)</span>, since we've noted that the objective becomes almost-everywhere flat in the limit<sup id="fnref:measure-theory"><a class="footnote-ref" href="#fn:measure-theory">1</a></sup> and thus becomes very difficult to optimize using typical methods. Instead, we optimize over the sequence of functions, for <span class="math">\(C_k \to \infty\)</span>,</p>
<div class="math">$$
x^{(k)} = \min_x\mathcal{L}(x; c, R, C_k) 
$$</div>
<p>picking only</p>
<div class="math">$$
x^* = \lim_{k\to\infty} x^{(k)}
$$</div>
<p>as our final trajectory. The goal of this post is to explore some methods for optimizing this function in the constrained, embedded environment which we'll be using for the competition.</p>
<h2>Gradient descent</h2>
<p>I'll do a quick introduction to gradient descent since there are <a href="https://medium.com/ai-society/hello-gradient-descent-ef74434bdfa5">plenty</a> of <a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/">posts</a> on <a href="https://www.quora.com/What-is-an-intuitive-explanation-of-gradient-descent">this</a> <a href="http://homes.soic.indiana.edu/classes/spring2012/csci/b553-hauserk/gradient_descent.pdf">method</a>, many of which I suspect are much better than anything I'll ever write.</p>
<p>Anyways, the simple idea (or another perspective on it) is that, if we want to find the minimum of a function <span class="math">\(V\)</span>, then we can think about the function as a potential for a particle, whose position we call <span class="math">\(x\)</span>, and then just run Newton's equation forward in time!</p>
<div class="math">$$
m\ddot x = -\nabla V(x)
$$</div>
<p>where <span class="math">\(\ddot x = \frac{d^2x}{dt^2}\)</span> (this is just a rewriting of <span class="math">\(F=ma=m\ddot x\)</span>, where our force is conservative).  You may notice a problem with this idea: well, if we land in a well, we'll continue oscillating... that is, there's literally no friction to stop us from just continuing past the minimum. So, let's add this in as a force proportional to the velocity (but pointing in the opposite direction), with friction coefficient <span class="math">\(\mu&gt;0\)</span>:</p>
<div class="math">$$
m\ddot x = -\nabla V(x) - \mu \dot x.
$$</div>
<p>Now, here I'll note we can do two things: one, we can keep the former term containing acceleration (i.e. momentum), accepting that we could possibly overshoot our minimum (because, say, we're going 'too fast') but then later 'come back' to it (this is known as gradient descent with momentum),<sup id="fnref:momentum"><a class="footnote-ref" href="#fn:momentum">2</a></sup> or, if we never want to overshoot it (but allow for the possibility that we may always be too slow in getting there in the first place) we can just send our momentum term to zero by sending <span class="math">\(m \to 0\)</span>. I'll take the latter approach for now, but we'll consider the former case, soon.</p>
<p>Anyways, sending <span class="math">\(m\to 0\)</span> corresponds to having a ball slowly rolling down an extremely sticky hill, stopping only at a local minimum, that is:</p>
<div class="math">$$
\mu\dot x + \nabla V(x) = 0
$$</div>
<p>or, in other words:</p>
<div class="math">$$
\dot x = -\frac{1}{\mu}\nabla V(x).
$$</div>
<p>Discretizing this equation by noting that, by definition of the derivative, we have</p>
<div class="math">$$
\dot x(t_{i+1}) \approx \frac{x_{i+1} - x_i}{h}
$$</div>
<p>then gives us (by plugging this into the above)</p>
<div class="math">$$
\frac{x_{i+1} - x_i}{h} = -\frac{1}{\mu}\nabla V(x),
$$</div>
<p>or, after rearranging (and setting <span class="math">\(\mu=1\)</span>, since we can control <span class="math">\(h\)</span> however we like, say by defining <span class="math">\(h := \frac{h}{\mu}\)</span>)</p>
<div class="math">$$
x_{i+1} = x_i - h\nabla V(x).
$$</div>
<p>In other words, gradient descent corresponds to the discretization of Newton's equations in the <em>overdamped</em> limit (e.g. in the limit of small mass and large friction).</p>
<p>This method is great because (a) we know it converges with probability 1 (as was relatively recently proven <a href="https://arxiv.org/abs/1602.04915">here</a>) for arbitrary, somewhat nice functions and (b) because it <em>works</em>. That being said, it's slow; for example, in the previous post, we saw that it converged after 5000 iterations (which, to be fair, takes about 20 seconds on my machine, but still).</p>
<p>A simple improvement (where we don't throw <span class="math">\(m\to 0\)</span>) yields a significant speed up! Of course, at the cost of having to deal with more hyperparameters, but that's okay: we're big kids now, and we can deal with more than one hyperparameter in our problems.</p>
<h2>Gradient descent with momentum</h2>
<p>The next idea is to, instead of taking <span class="math">\(m\to 0\)</span>, just write out the full discretization scheme. To make our lives easier, we rewrite <span class="math">\(v(t) \equiv \dot x(t)\)</span> to be our velocity, this gives us a simple rewriting of the form:</p>
<div class="math">$$
\begin{align}
m\dot v &amp;= -\nabla V(x) - \mu v\\
\dot x(t) &amp;= v(t)
\end{align}
$$</div>
<p>discretizing the second equation with some step-size <span class="math">\(h'\)</span> (as above) we get</p>
<div class="math">$$
x_{t+1} = x_t + h'v_{t+1}
$$</div>
<p>where the former equation is, when discretized with some step size <span class="math">\(h\)</span></p>
<div class="math">$$
m\frac{v_{t+1} - v_t}{h} = -\nabla V(x_t) - \mu v_t
$$</div>
<p>or after rearranging, and defining <span class="math">\(\gamma \equiv \frac{h}{m}\)</span> (which we can make as small as we'd like)</p>
<div class="math">$$
\begin{align}
v_{t+1} &amp;= -\gamma \nabla V(x_t) + (1-\mu \gamma) v_t\\
x_{t+1} &amp;= x_t + h'v_{t+1}
\end{align}
$$</div>
<p>usually we take <span class="math">\(h' = 1\)</span>, and, to prevent <span class="math">\(v_t\)</span> from having weird behaviour, we require that <span class="math">\(1-\mu\gamma &gt; 0\)</span>, i.e. that <span class="math">\(\gamma &lt; \frac{1}{\mu}\)</span>.<sup id="fnref:oscillations"><a class="footnote-ref" href="#fn:oscillations">3</a></sup> If we call <span class="math">\(\beta \equiv 1 - \mu\gamma\)</span> and therefore have that <span class="math">\(0 &lt; \beta  &lt; 1\)</span> then we obtain the classical momentum for gradient descent</p>
<div class="math">$$
\begin{align}
v_{t+1} &amp;= -\gamma \nabla V(x_t) + \beta v_t\\
x_{t+1} &amp;= x_t + v_{t+1}
\end{align}
$$</div>
<p>which is what we needed! Well... close to what we needed, really.</p>
<p>Anyways, just to give some perspective on the speed up: using momentum, the optimization problem took around 600 iterations to converge, more than 8 times less than the original given above. I'll give a picture of this soon, but I'm missing one more slight detail.</p>
<h2>Cooling schemes</h2>
<p>Imagine we want to optimize some function <span class="math">\(\ell(\cdot)\)</span> that is, in general, extremely hard to solve. If we're lucky, we may be able to do the next best thing: take a series of functions parametrized by, say, <span class="math">\(C\)</span>, such that <span class="math">\(\ell_C(\cdot) \to \ell(\cdot)\)</span> as <span class="math">\(C\to\infty\)</span>,<sup id="fnref:approaches"><a class="footnote-ref" href="#fn:approaches">4</a></sup> <em>and</em> where the problem is simple to solve for <span class="math">\(C_{k+1}\)</span>, given the solution for <span class="math">\(C_k\)</span>.</p>
<p>Of course, given this and the above we can already solve the problem: we begin with some small <span class="math">\(C_0\)</span> and then, after converging for <span class="math">\(\ell_{C_0}(\cdot)\)</span> we then continue to <span class="math">\(\ell_{C_1}(\cdot)\)</span>, after converging to that, we then continue to solve for <span class="math">\(\ell_{C_2}(\cdot)\)</span>, etc., until we reach some desired tolerance on the given result.</p>
<p>Or... (of course, I'm writing this for a reason), we could do something fancy using the previous scheme:</p>
<p>Every time we update our variable, we also increase <span class="math">\(k\)</span> such that both the problem sequence and the final solution converge at the same time. It is, of course, totally not obvious that this works (though with some decent choice of schedule, one could imagine it should); the video below shows this idea in action using both momentum and this particular choice of cooling scheme (note the number of iterations is much lower relative to the previous attempt's 5000, but also note that, while the scheme converged in the norm‚Äîthat is, the variables were updated very little‚Äîit didn't actually converge to an optimal solution, but it was pretty close!).</p>
<p>I'd highly recommend looking at the code in order to get a better understanding of how all this is implemented and the dirty deets.</p>
<p>Anyways, optimizing the original likelihood presented in the first post (and in the first part of <em>this</em> post) using momentum and the above cooling schedule yields the following nice little video:</p>
<video controls>
    <source src="/images/path-optimization-2/path_optimization_2.mp4" type="video/mp4">
</video>

<p>As before, this code (with more details and implementation) can be found in the <a href="https://github.com/StanfordAIR/optimization-sandbox">StanfordAIR Github repo</a>.</p>
<!-- Footnotes -->

<div class="footnote">
<hr>
<ol>
<li id="fn:measure-theory">
<p>This is, indeed, a technical term, but it's also quite suggestive of what really happens.&#160;<a class="footnote-backref" href="#fnref:measure-theory" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:momentum">
<p>Of course, there are many reasons why we'll want momentum, but those will come soon.&#160;<a class="footnote-backref" href="#fnref:momentum" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:oscillations">
<p>Consider <span class="math">\(V(x) = 0\)</span>, with some initial condition, <span class="math">\(v_0 &gt; 0\)</span>, say, then we'll have
<div class="math">$$
v_t = -k v_{t-1}
$$</div>
for some <span class="math">\(k = \mu\gamma - 1&gt;0\)</span>. Solving this yields <span class="math">\(v_{t} = (-1)^tk^t v_{0}\)</span>. This is weird, because it means that our velocity will change directions every iteration even though there's no potential! This is definitely not expected (nor desirable) behaviour.&#160;<a class="footnote-backref" href="#fnref:oscillations" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:approaches">
<p>In some sense. Say: in the square error, or something of the like. This can be made entirely rigorous, but I choose not to do it here since it's not terribly essential.&#160;<a class="footnote-backref" href="#fnref:approaches" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        <hr>
        <div class="socials">
            <li>
                <div class="small-img">
                <a href="https://twitter.com/intent/tweet?screen_name=guilleangeris">
                <img src="/theme/img_static/Twitter_Social_Icon_Circle_Color.png"
                    style="width:1.5em" alt="Twitter logo"></a>
                </div>
                <div class="small-img">
                <a href="https://github.com/angeris">
                <img src="/theme/img_static/GitHub-Mark-120px-plus.png"
                     style="width:1.5em" alt="Github logo"></a>
                </div>
            </li>
        </div>
    </div>
</div>
    </div>
</body>
<footer>
    Made with <a href="https://blog.getpelican.com">üêç</a> and hosted on <a href="https://neocities.org">neocities.org</a>.
</footer>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-73543332-2', 'auto');
    ga('send', 'pageview');
</script>
</html>