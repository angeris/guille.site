<!DOCTYPE html>
<html lang="en">
<head>
    <title>Least squares and image processing</title>
    
    <link rel="stylesheet" href="/theme/css/main.css">
    <link href="https://fonts.googleapis.com/css?family=Raleway:100,300,500" rel="stylesheet">

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ['\\(','\\)'] ]
            }
        });
    </script>
    
    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">    	

</head>
<body>
    <div class="container">
<div class="row">
    <div class="col-md-8">
        <h2><a href="/">&larr; Home</a></h2>
        <h1>Least squares and image processing</h1>
        <p>Category: <a href="/category/math.html">math</a></p>
        <p><label>Posted <strong>September 16, 2017</strong></label></p>
        <p>Least squares is one of those things that seems relatively simple once you first look at it (perhaps also because most linear algebra texts relegate it to nothing more than a page or two on their textbooks), but has surprisingly powerful implications that are quite nice, and, most importantly, that are easily computable.</p>
<p>If you're interested in looking at the results first, I'd recommend skipping the following section and going immediately to the next one, which shows the application.</p>
<p>So, I guess we'll dive right in, right?</p>
<h2>Vanilla least squares</h2>
<p>The usual least-squares many of us have heard of is a problem of the form</p>
<p>$$
\min_x \,\,\lVert Ax - b \lVert^2
$$</p>
<p>where I define $\lVert y\lVert^2 \equiv \sum_i y_i^2 = y^Ty$ to be the usual Euclidean norm. This problem has a unique solution provided that $A$ is full-rank (i.e. has independent columns), and therefore that $A^TA$ is invertible.<sup id="fnref-sq-invertible"><a class="footnote-ref" href="#fn-sq-invertible">1</a></sup> This is true since the problem above is convex (e.g. any local minimum, if it exists, corresponds to the global minimum<sup id="fnref-convex-global-min"><a class="footnote-ref" href="#fn-convex-global-min">2</a></sup>), coercive (the function diverges to infinity, and therefore <em>has</em> a local minimum) and differentiable such that</p>
<p>$$
\nabla \lVert Ax - b \lVert^2 = \nabla (Ax-b)^T(Ax-b) = 2A^T(Ax-b) = 0,
$$</p>
<p>or that</p>
<p>$$
A^TAx = A^Tb.
$$</p>
<p>This equation is called the <em>normal equation</em>, which has a unique solution for $x$ since we said $A^TA$ is invertible. In other words, we can write down the (surprisingly, less useful) equation for $x$</p>
<p>$$
x = (A^TA)^{-1}A^Tb.
$$</p>
<p>A simple example of direct least squares can be found on the <a href="/pid-ls.html">previous post</a>, but that's nowhere as interesting as an actual example, using some images. First, I should note that this can be immediately extended to cover (multi-)objectives of the form, for $\lambda_i &gt; 0$</p>
<p>$$
\min_x \,\,\lambda_1\lVert A_1x - b_1 \lVert^2 + \lambda_2\lVert A_2x - b_2 \lVert^2 + \dots + \lambda_n\lVert A_nx - b_n \lVert^2
$$</p>
<p>by noting that (say, with two variables, though the idea extends to any number of objectives), by pulling the $\lambda_i$ into the inside of the norm</p>
<p>$$
\lambda_1\lVert A_1x - b_1 \lVert^2 + \lambda_2\lVert A_2x - b_2 \lVert^2  = \left\lVert 
\begin{bmatrix}
\sqrt{\lambda_1} A_1\\
\sqrt{\lambda_2} A_2
\end{bmatrix}
x
-
\begin{bmatrix}
\sqrt{\lambda_1}b_1\\
\sqrt{\lambda_2}b_2
\end{bmatrix}\right\lVert^2.
$$</p>
<p>Where the new matrices above are defined as the 'stacked' (appended) matrix of $A_1, A_2$ and the 'stacked' vector $b_1, b_2$. Or, defining </p>
<p>$$
\bar A \equiv 
\begin{bmatrix}
\sqrt{\lambda_1} A_1\\
\sqrt{\lambda_2} A_2
\end{bmatrix}
$$</p>
<p>and</p>
<p>$$
\bar b \equiv
\begin{bmatrix}
\sqrt{\lambda_1} b_1\\
\sqrt{\lambda_2} b_2
\end{bmatrix}
$$</p>
<div class="footnote">
<hr>
<ol>
<li id="fn-sq-invertible">
<p>If $A^TA x = 0$ then $x^TA^TAx = 0$, but $x^TA^TAx = (Ax)^T(Ax) = \lVert Ax \lVert^2 = 0$ which is zero only when $Ax = 0$. E.g. only if there is an $x$ in the nullspace of $A$.&#160;<a class="footnote-backref" href="#fnref-sq-invertible" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn-convex-global-min">
<p>A proof is straightforward. Let's say $f$ is differentiable, since this is the case we care about, then we say $f$ is convex if the hyperplane given by $\nabla f(x)$ bounds $f$ from below. A nice picture usually helps with this:&#160;<a class="footnote-backref" href="#fnref-convex-global-min" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>
        <hr>
        <div class="socials">
            <li>Got any questions?</li>
            <li><a href="https://twitter.com/intent/tweet?screen_name=guilleangeris" class="twitter-mention-button" data-size="large" data-show-count="false">Tweet to @guilleangeris</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></li>
        </div>
    </div>
</div>
    </div>
</body>
<footer>
    Made with <a href="https://blog.getpelican.com">üêç</a> and hosted on <a href="https://neocities.org">neocities.org</a>.
</footer>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-73543332-2', 'auto');
    ga('send', 'pageview');
</script>
</html>